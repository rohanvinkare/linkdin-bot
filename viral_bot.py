import requests
import feedparser
import json
import random
import time
import os
import datetime
import re
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
HISTORY_FILE = "history.json"
LINKEDIN_PERSON_URN = os.environ.get("LINKEDIN_URN", "").strip()
LINKEDIN_ACCESS_TOKEN = os.environ.get("LINKEDIN_TOKEN", "").strip()
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "").strip()

# --- FALLBACK IMAGES ---
FALLBACK_IMAGES = [
    "https://images.unsplash.com/photo-1558494949-ef526b0042a0", 
    "https://images.unsplash.com/photo-1518770660439-4636190af475", 
    "https://images.unsplash.com/photo-1555099962-4199c345e5dd", 
    "https://images.unsplash.com/photo-1531403009284-440f080d1e12", 
    "https://images.unsplash.com/photo-1451187580459-43490279c0fa"  
]

# --- SOURCES ---
NEWS_FEEDS = [
    "https://feeds.feedburner.com/TheHackersNews", 
    "https://techcrunch.com/feed/",
    "https://www.theverge.com/rss/index.xml"
]

ENGINEERING_FEEDS = [
    "https://netflixtechblog.com/feed",           
    "https://eng.uber.com/feed/",                 
    "https://aws.amazon.com/blogs/architecture/feed/", 
    "https://blog.bytebytego.com/feed",           
    "https://martinfowler.com/feed.atom",         
    "https://slack.engineering/feed/",            
    "https://engineering.fb.com/feed/",
    "https://shopify.engineering/blog/feed",
    "https://engineering.linkedin.com/blog.rss"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- UNICODE TEXT STYLING FOR LINKEDIN ---
def apply_unicode_styling(text):
    """
    Converts text to Unicode styled variants since LinkedIn doesn't support markdown.
    Uses mathematical alphanumeric symbols for bold/italic effects.
    """
    # Unicode character mappings for styled text
    BOLD_MAP = {
        'A': 'ùóî', 'B': 'ùóï', 'C': 'ùóñ', 'D': 'ùóó', 'E': 'ùóò', 'F': 'ùóô', 'G': 'ùóö', 'H': 'ùóõ', 'I': 'ùóú', 'J': 'ùóù',
        'K': 'ùóû', 'L': 'ùóü', 'M': 'ùó†', 'N': 'ùó°', 'O': 'ùó¢', 'P': 'ùó£', 'Q': 'ùó§', 'R': 'ùó•', 'S': 'ùó¶', 'T': 'ùóß',
        'U': 'ùó®', 'V': 'ùó©', 'W': 'ùó™', 'X': 'ùó´', 'Y': 'ùó¨', 'Z': 'ùó≠',
        'a': 'ùóÆ', 'b': 'ùóØ', 'c': 'ùó∞', 'd': 'ùó±', 'e': 'ùó≤', 'f': 'ùó≥', 'g': 'ùó¥', 'h': 'ùóµ', 'i': 'ùó∂', 'j': 'ùó∑',
        'k': 'ùó∏', 'l': 'ùóπ', 'm': 'ùó∫', 'n': 'ùóª', 'o': 'ùóº', 'p': 'ùóΩ', 'q': 'ùóæ', 'r': 'ùóø', 's': 'ùòÄ', 't': 'ùòÅ',
        'u': 'ùòÇ', 'v': 'ùòÉ', 'w': 'ùòÑ', 'x': 'ùòÖ', 'y': 'ùòÜ', 'z': 'ùòá',
        '0': 'ùü¨', '1': 'ùü≠', '2': 'ùüÆ', '3': 'ùüØ', '4': 'ùü∞', '5': 'ùü±', '6': 'ùü≤', '7': 'ùü≥', '8': 'ùü¥', '9': 'ùüµ'
    }
    
    def make_bold(match):
        text = match.group(1)
        return ''.join(BOLD_MAP.get(c, c) for c in text)
    
    # Convert [BOLD:text] markers to Unicode bold
    text = re.sub(r'\[BOLD:(.*?)\]', make_bold, text)
    
    return text

def clean_text_for_linkedin(text):
    """Enhanced text cleaning with Unicode styling support"""
    # Remove markdown artifacts
    text = text.replace("**", "").replace("__", "").replace("##", "")
    
    # Replace bullet points with stylish alternatives
    text = re.sub(r'^[\*\-]\s+', '‚ñ∏ ', text, flags=re.MULTILINE)
    
    # Clean up excessive newlines
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # Apply Unicode styling for emphasis
    text = apply_unicode_styling(text)
    
    return text.strip()

# --- UTILS ---
def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r") as f: return json.load(f)
        except: return []
    return []

def save_history(history_data, title, link):
    entry = {"title": title, "web_link": link.split("?")[0], "date": datetime.datetime.now().strftime("%Y-%m-%d")}
    history_data.append(entry)
    if len(history_data) > 200: history_data = history_data[-200:]
    with open(HISTORY_FILE, "w") as f: json.dump(history_data, f, indent=4)

def is_already_posted(link, title, history_data):
    clean_link = link.split("?")[0]
    for entry in history_data:
        if entry.get("web_link") == clean_link or entry.get("title") == title: return True
    return False

# --- DYNAMIC MODEL SELECTOR ---
def get_valid_model_name():
    """Asks Google which models are actually available to avoid 404s"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GEMINI_API_KEY}"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            available_models = [
                m['name'].replace("models/", "") 
                for m in data.get('models', []) 
                if 'generateContent' in m.get('supportedGenerationMethods', [])
            ]
            
            # Prefer Flash, then Pro, then anything else
            if "gemini-1.5-flash" in available_models: return "gemini-1.5-flash"
            if "gemini-1.5-flash-latest" in available_models: return "gemini-1.5-flash-latest"
            if "gemini-1.0-pro" in available_models: return "gemini-1.0-pro"
            
            if available_models:
                print(f"   ‚ö†Ô∏è Preferred model not found. Using fallback: {available_models[0]}")
                return available_models[0]
                
    except Exception as e:
        print(f"   ‚ö†Ô∏è Could not fetch model list: {e}")
    
    return "gemini-1.5-flash-latest"

# --- ENHANCED IMAGE SCRAPER ---
def get_article_details(url, mode):
    try:
        print(f"   ‚¨áÔ∏è  Scraping: {url}")
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.content, 'html.parser')
        
        possible_bodies = soup.select('article, .post-content, .entry-content, #article-body, .gh-content')
        target = possible_bodies[0] if possible_bodies else soup
        text = "\n".join([p.get_text().strip() for p in target.find_all(['p', 'h2'])])
        
        if len(text) < 600: return None
        
        # ENHANCED IMAGE EXTRACTION - Multiple strategies
        image_url = None
        
        # Strategy 1: Twitter/X card image (often best quality)
        twitter_img = soup.find("meta", attrs={"name": "twitter:image"})
        if twitter_img and twitter_img.get("content"):
            image_url = twitter_img["content"]
            print(f"   üñºÔ∏è  Found Twitter Card image")
        
        # Strategy 2: Open Graph image
        if not image_url:
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"): 
                image_url = og_image["content"]
                print(f"   üñºÔ∏è  Found OG image")
        
        # Strategy 3: Look for hero/featured images in article
        if not image_url:
            hero_selectors = [
                'img.featured-image', 'img.hero-image', 'img.post-image',
                'figure.featured img', 'div.featured-image img',
                'div.post-thumbnail img', 'div.entry-image img'
            ]
            for selector in hero_selectors:
                hero_img = target.select_one(selector)
                if hero_img and hero_img.get('src'):
                    src = hero_img['src']
                    if src.startswith('http'):
                        image_url = src
                        print(f"   üñºÔ∏è  Found hero image")
                        break
        
        # Strategy 4: First large image in content (width/height check)
        if not image_url:
            for img in target.find_all('img'):
                src = img.get('src', '')
                width = img.get('width', '0')
                height = img.get('height', '0')
                
                # Convert to int safely
                try:
                    w = int(width) if width and str(width).isdigit() else 0
                    h = int(height) if height and str(height).isdigit() else 0
                except:
                    w, h = 0, 0
                
                # Prioritize larger images (likely feature images, not icons)
                if src.startswith('http') and (w > 400 or h > 300 or (w == 0 and h == 0)):
                    # Avoid logos, icons, tracking pixels
                    if not any(skip in src.lower() for skip in ['logo', 'icon', 'avatar', 'pixel', 'tracking', '1x1']):
                        image_url = src
                        print(f"   üñºÔ∏è  Found content image ({w}x{h})")
                        break
        
        # Strategy 5: Fallback to first valid image
        if not image_url:
            first_img = target.find('img')
            if first_img and first_img.get('src'):
                src = first_img['src']
                if src.startswith('http'): 
                    image_url = src
                    print(f"   üñºÔ∏è  Using first available image")
        
        # Strategy 6: Use fallback for concepts only
        if not image_url and mode == "CONCEPT":
            print("   ‚ö†Ô∏è No image found. Using Fallback.")
            image_url = random.choice(FALLBACK_IMAGES)
        
        if not image_url:
            print("   ‚ö†Ô∏è No suitable image found. SKIP.")
            return None 
            
        return {"text": text[:12000], "image": image_url}
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error: {e}")
        return None

def fetch_content(history_data):
    # CHANGED: Prioritize concepts (85% concepts, 15% news)
    mode = "CONCEPT" if random.random() > 0.15 else "NEWS"
    sources = ENGINEERING_FEEDS if mode == "CONCEPT" else NEWS_FEEDS
    random.shuffle(sources)
    
    print(f"üé≤ Mode Selected: {mode}")
    
    for feed_url in sources:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:3]:
                if is_already_posted(entry.link, entry.title, history_data): continue
                details = get_article_details(entry.link, mode)
                if not details: continue 
                return {
                    "type": mode,
                    "title": entry.title,
                    "link": entry.link,
                    "full_text": details["text"],
                    "image_url": details["image"]
                }
        except: continue
    return None

# --- AI WRITER WITH UNICODE STYLING ---
def generate_viral_post(content_item):
    valid_model = get_valid_model_name()
    print(f"   üß† Asking Gemini ({valid_model})...")
    
    if not GEMINI_API_KEY:
        print("   ‚ùå ERROR: GEMINI_API_KEY is missing!")
        return None

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{valid_model}:generateContent?key={GEMINI_API_KEY}"
    
    base_structure = """
    CRITICAL FORMATTING RULES FOR LINKEDIN:
    1. NO Markdown syntax (**, __, ##) - LinkedIn doesn't support it
    2. Use [BOLD:text] for emphasis - will be converted to Unicode bold
    3. Use these emojis for structure: üí° üîπ üëâ üöÄ ‚ö†Ô∏è ‚ö° üéØ
    4. Use line breaks and spacing for readability
    5. Keep it visually clean and scannable
    """
    
    if content_item['type'] == "CONCEPT":
        prompt = f"""
        Act as a Principal Staff Engineer writing for LinkedIn.
        Topic: {content_item['title']}
        Context: {content_item['full_text'][:5000]}
        {base_structure}
        
        OUTPUT FORMAT (use exact structure):
        
        [BOLD:Counter-intuitive Hook Question]
        
        [Brief 2-sentence context explaining the problem/concept]
        
        üí° [BOLD:The Architecture:]
        ‚ñ∏ [Key architectural decision 1]
        ‚ñ∏ [Key architectural decision 2]
        ‚ñ∏ [Key architectural decision 3]
        
        ‚ö° [BOLD:The Trade-offs:]
        ‚ö†Ô∏è Risk: [Main challenge/limitation]
        üöÄ Benefit: [Main advantage/win]
        
        üéØ [BOLD:Bottom Line:]
        [Sharp 1-line conclusion]
        
        üîó Read more: {content_item['link']}
        
        #systemdesign #softwarearchitecture #engineering
        """
    else: 
        prompt = f"""
        Act as a Tech Lead writing LinkedIn news commentary.
        News: {content_item['title']}
        Context: {content_item['full_text'][:4000]}
        {base_structure}
        
        OUTPUT FORMAT (use exact structure):
        
        [Provocative hook statement]
        
        üëâ [BOLD:The TL;DR:]
        ‚ñ∏ [Key fact 1]
        ‚ñ∏ [Key fact 2]
        ‚ñ∏ [Key fact 3]
        
        üéØ [BOLD:Why Engineers Should Care:]
        ‚ñ∏ [Engineering impact]
        ‚ñ∏ [Market/industry impact]
        
        [Sharp cynical 1-line take]
        
        üîó Source: {content_item['link']}
        
        #tech #technews #engineering
        """

    try:
        resp = requests.post(url, headers={"Content-Type": "application/json"}, json={"contents": [{"parts": [{"text": prompt}]}]})
        
        if resp.status_code != 200:
            print(f"   ‚ùå GOOGLE API ERROR {resp.status_code}: {resp.text}")
            return None

        text = resp.json()['candidates'][0]['content']['parts'][0]['text']
        return clean_text_for_linkedin(text)

    except Exception as e:
        print(f"   ‚ùå EXCEPTION: {e}")
    return None

# --- POSTER ---
def post_to_linkedin(content, image_url):
    print(f"   üì§ Uploading Image: {image_url}")
    try:
        img_data = requests.get(image_url, headers=HEADERS, timeout=10).content
    except:
        print("   ‚ùå Failed to download image.")
        return False
    
    reg = requests.post(
        "https://api.linkedin.com/v2/assets?action=registerUpload",
        headers={"Authorization": f"Bearer {LINKEDIN_ACCESS_TOKEN}"},
        json={
            "registerUploadRequest": {
                "recipes": ["urn:li:digitalmediaRecipe:feedshare-image"],
                "owner": LINKEDIN_PERSON_URN,
                "serviceRelationships": [{"relationshipType": "OWNER", "identifier": "urn:li:userGeneratedContent"}]
            }
        }
    )
    
    if reg.status_code != 200: 
        print(f"   ‚ùå LinkedIn Upload Error: {reg.text}")
        return False
    
    upload_url = reg.json()['value']['uploadMechanism']['com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest']['uploadUrl']
    asset = reg.json()['value']['asset']
    
    up = requests.put(upload_url, data=img_data, headers={"Authorization": f"Bearer {LINKEDIN_ACCESS_TOKEN}"})
    if up.status_code != 201: return False
    
    post_body = {
        "author": LINKEDIN_PERSON_URN,
        "lifecycleState": "PUBLISHED",
        "specificContent": {
            "com.linkedin.ugc.ShareContent": {
                "shareCommentary": {"text": content},
                "shareMediaCategory": "IMAGE",
                "media": [{"status": "READY", "media": asset}]
            }
        },
        "visibility": {"com.linkedin.ugc.MemberNetworkVisibility": "PUBLIC"}
    }
    
    final = requests.post("https://api.linkedin.com/v2/ugcPosts", headers={"Authorization": f"Bearer {LINKEDIN_ACCESS_TOKEN}"}, json=post_body)
    return final.status_code == 201

# --- MAIN ---
if __name__ == "__main__":
    print("ü§ñ Bot Started...")
    history = load_history()
    posted_successfully = False
    
    for attempt in range(1, 6):
        if posted_successfully: break
        print(f"\n--- Attempt {attempt}/5 ---")
        
        content = fetch_content(history)
        if not content: 
            print("   -> No content found.")
            continue 
            
        post_text = generate_viral_post(content)
        if not post_text: 
            continue 
        
        print("\n--- PREVIEW ---")
        print(post_text)
        print("--- END PREVIEW ---")
        
        if post_to_linkedin(post_text, content['image_url']):
             print("‚úÖ Posted Successfully!")
             save_history(history, content['title'], content['link'])
             posted_successfully = True
        else:
             print("‚ùå API Post failed. Retrying...")
             time.sleep(5)